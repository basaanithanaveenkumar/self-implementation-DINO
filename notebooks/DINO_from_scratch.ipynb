{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNG/8zesiUF25vLB7zCrQmh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basaanithanaveenkumar/self-implementation-DINO/blob/main/notebooks/DINO_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The EMA update rule for a parameter vector is:\n",
        "\n",
        "$$\n",
        "\\theta_{\\text{teacher}} \\gets m \\times \\theta_{\\text{teacher}} + (1 - m) \\times \\theta_{\\text{student}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\theta_{\\text{teacher}}$: Teacher model parameters\n",
        "- $\\theta_{\\text{student}}$: Student model parameters  \n",
        "- $m$: Momentum coefficient (typically close to 1, e.g., 0.99, 0.996)"
      ],
      "metadata": {
        "id": "KzMtWIIyJ4zn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why EMA is Used in DINO\n",
        "\n",
        "## 1. Stable Targets\n",
        "The teacher network provides consistent, slowly evolving targets for the student to learn from.\n",
        "\n",
        "## 2. Prevents Collapse\n",
        "EMA helps avoid the trivial solution where both networks output constant representations.\n",
        "\n",
        "## 3. Improved Generalization\n",
        "The teacher acts as an ensemble of previous student models, capturing robust features./"
      ],
      "metadata": {
        "id": "4T6FJZeqKEGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DINO_MLP_HD(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hidden_dim=2048, bottleneck_dim=1256,\n",
        "                 n_layers=4, use_layer_norm=True,):\n",
        "        super().__init__()\n",
        "        # Build the MLP layers\n",
        "        layers = []\n",
        "\n",
        "        # Input layer\n",
        "        layers.append(nn.Linear(in_dim, hidden_dim))\n",
        "        if use_bn:\n",
        "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "        layers.append(nn.GELU())\n",
        "\n",
        "        # Hidden layers\n",
        "        for _ in range(n_layers - 2):\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            if use_layer_norm:\n",
        "                layers.append(nn.LayerNorm(hidden_dim))\n",
        "            layers.append(nn.GELU())\n",
        "\n",
        "        # Bottleneck layer\n",
        "        layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
        "\n",
        "        # Create the MLP\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "        self.last_layer=nn.linear(bottleneck_dim,out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        x=self.last_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "FApdWY0l4PlR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "# vibe coding\n",
        "\n",
        "\n",
        "class DINO_MLP_HD(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hidden_dim=2048, bottleneck_dim=1256,\n",
        "                 n_layers=4, use_layer_norm=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Build the MLP layers\n",
        "        layers = []\n",
        "\n",
        "        # Input layer\n",
        "        layers.append(nn.Linear(in_dim, hidden_dim))\n",
        "        if use_layer_norm:\n",
        "            layers.append(nn.LayerNorm(hidden_dim))\n",
        "        layers.append(nn.GELU())\n",
        "\n",
        "        # Hidden layers\n",
        "        for _ in range(n_layers - 2):\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            if use_layer_norm:\n",
        "                layers.append(nn.LayerNorm(hidden_dim))\n",
        "            layers.append(nn.GELU())\n",
        "\n",
        "        # Bottleneck layer\n",
        "        layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
        "\n",
        "        # Create the MLP\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "        # Last layer (corrected from nn.linear to nn.Linear)\n",
        "        self.last_layer = nn.Linear(bottleneck_dim, out_dim)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            # Use Xavier initialization for linear layers\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        x = self.last_layer(x)\n",
        "        return x\n",
        "\n",
        "    def print_summary(self, input_size=(1, 256)):\n",
        "        \"\"\"\n",
        "        Print model summary similar to torchsummary\n",
        "        \"\"\"\n",
        "        print(\"=\" * 80)\n",
        "        print(\"DINO MLP Head Summary\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Input size: {input_size}\")\n",
        "        print(f\"Output size: {input_size[0]}, {self.last_layer.out_features}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        total_params = 0\n",
        "        trainable_params = 0\n",
        "\n",
        "        print(f\"{'Layer (type)':<25} {'Output Shape':<20} {'Param #':<15} {'Trainable':<10}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Create a dummy input\n",
        "        x = torch.randn(input_size)\n",
        "\n",
        "        # Track layers\n",
        "        layers_info = []\n",
        "\n",
        "        # Process through MLP\n",
        "        for i, layer in enumerate(self.mlp):\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                layer_type = f\"Linear_{i//3+1}\"\n",
        "                param_count = sum(p.numel() for p in layer.parameters())\n",
        "                total_params += param_count\n",
        "                trainable_params += param_count\n",
        "\n",
        "                # Get output shape\n",
        "                x = layer(x)\n",
        "                output_shape = list(x.shape)\n",
        "\n",
        "                layers_info.append({\n",
        "                    'name': layer_type,\n",
        "                    'output_shape': output_shape.copy(),\n",
        "                    'params': param_count,\n",
        "                    'trainable': True\n",
        "                })\n",
        "            elif isinstance(layer, nn.LayerNorm):\n",
        "                layer_type = f\"LayerNorm_{i//3+1}\"\n",
        "                param_count = sum(p.numel() for p in layer.parameters())\n",
        "                total_params += param_count\n",
        "                trainable_params += param_count\n",
        "\n",
        "                # Get output shape\n",
        "                x = layer(x)\n",
        "                output_shape = list(x.shape)\n",
        "\n",
        "                layers_info.append({\n",
        "                    'name': layer_type,\n",
        "                    'output_shape': output_shape.copy(),\n",
        "                    'params': param_count,\n",
        "                    'trainable': True\n",
        "                })\n",
        "            elif isinstance(layer, nn.GELU):\n",
        "                layer_type = f\"GELU_{i//3+1}\"\n",
        "                param_count = 0\n",
        "\n",
        "                # Get output shape\n",
        "                x = layer(x)\n",
        "                output_shape = list(x.shape)\n",
        "\n",
        "                layers_info.append({\n",
        "                    'name': layer_type,\n",
        "                    'output_shape': output_shape.copy(),\n",
        "                    'params': param_count,\n",
        "                    'trainable': False\n",
        "                })\n",
        "\n",
        "        # Process last layer\n",
        "        layer_type = \"Last_Linear\"\n",
        "        param_count = sum(p.numel() for p in self.last_layer.parameters())\n",
        "        total_params += param_count\n",
        "        trainable_params += param_count\n",
        "\n",
        "        x = self.last_layer(x)\n",
        "        output_shape = list(x.shape)\n",
        "\n",
        "        layers_info.append({\n",
        "            'name': layer_type,\n",
        "            'output_shape': output_shape.copy(),\n",
        "            'params': param_count,\n",
        "            'trainable': True\n",
        "        })\n",
        "\n",
        "        # Print all layers\n",
        "        for info in layers_info:\n",
        "            print(f\"{info['name']:<25} {str(info['output_shape']):<20} {info['params']:<15} {info['trainable']:<10}\")\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Total params: {total_params:,}\")\n",
        "        print(f\"Trainable params: {trainable_params:,}\")\n",
        "        print(f\"Non-trainable params: {total_params - trainable_params:,}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        return x.shape  # Return final output shape\n",
        "\n",
        "\n",
        "# Test the model\n",
        "if __name__ == \"__main__\":\n",
        "    # Create model instance\n",
        "    in_dim = 256\n",
        "    out_dim = 1024\n",
        "    model = DINO_MLP_HD(in_dim, out_dim, n_layers=4, use_layer_norm=True)\n",
        "\n",
        "    # Print model summary\n",
        "    print(\"Model Architecture:\")\n",
        "    print(model)\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    # Print detailed summary\n",
        "    final_shape = model.print_summary(input_size=(1, in_dim))\n",
        "\n",
        "    # Test forward pass\n",
        "    print(\"\\nTesting forward pass...\")\n",
        "    batch_size = 4\n",
        "    test_input = torch.randn(batch_size, in_dim)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(test_input)\n",
        "\n",
        "    print(f\"Input shape: {test_input.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Output range: [{output.min().item():.4f}, {output.max().item():.4f}]\")\n",
        "    print(f\"Output mean: {output.mean().item():.4f}\")\n",
        "    print(f\"Output std: {output.std().item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZWPQpXN6cpw",
        "outputId": "94e54c1c-7c89-4d35-f58a-d4853a82d49c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Architecture:\n",
            "DINO_MLP_HD(\n",
            "  (mlp): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=2048, bias=True)\n",
            "    (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "    (2): GELU(approximate='none')\n",
            "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "    (4): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "    (5): GELU(approximate='none')\n",
            "    (6): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "    (7): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "    (8): GELU(approximate='none')\n",
            "    (9): Linear(in_features=2048, out_features=1256, bias=True)\n",
            "  )\n",
            "  (last_layer): Linear(in_features=1256, out_features=1024, bias=True)\n",
            ")\n",
            "\n",
            "==================================================\n",
            "\n",
            "================================================================================\n",
            "DINO MLP Head Summary\n",
            "================================================================================\n",
            "Input size: (1, 256)\n",
            "Output size: 1, 1024\n",
            "--------------------------------------------------------------------------------\n",
            "Layer (type)              Output Shape         Param #         Trainable \n",
            "================================================================================\n",
            "Linear_1                  [1, 2048]            526336          1         \n",
            "LayerNorm_1               [1, 2048]            4096            1         \n",
            "GELU_1                    [1, 2048]            0               0         \n",
            "Linear_2                  [1, 2048]            4196352         1         \n",
            "LayerNorm_2               [1, 2048]            4096            1         \n",
            "GELU_2                    [1, 2048]            0               0         \n",
            "Linear_3                  [1, 2048]            4196352         1         \n",
            "LayerNorm_3               [1, 2048]            4096            1         \n",
            "GELU_3                    [1, 2048]            0               0         \n",
            "Linear_4                  [1, 1256]            2573544         1         \n",
            "Last_Linear               [1, 1024]            1287168         1         \n",
            "================================================================================\n",
            "Total params: 12,792,040\n",
            "Trainable params: 12,792,040\n",
            "Non-trainable params: 0\n",
            "================================================================================\n",
            "\n",
            "Testing forward pass...\n",
            "Input shape: torch.Size([4, 256])\n",
            "Output shape: torch.Size([4, 1024])\n",
            "Output range: [-3.1083, 2.7172]\n",
            "Output mean: 0.0070\n",
            "Output std: 0.7602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "import copy\n",
        "from typing import List, Optional\n",
        "\n",
        "class DINO_MLP_HD(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hidden_dim=2048, bottleneck_dim=256,\n",
        "                 n_layers=4, use_layer_norm=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Build the MLP layers\n",
        "        layers = []\n",
        "\n",
        "        # Input layer\n",
        "        layers.append(nn.Linear(in_dim, hidden_dim))\n",
        "        if use_layer_norm:\n",
        "            layers.append(nn.LayerNorm(hidden_dim))\n",
        "        layers.append(nn.GELU())\n",
        "\n",
        "        # Hidden layers\n",
        "        for _ in range(n_layers - 2):\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            if use_layer_norm:\n",
        "                layers.append(nn.LayerNorm(hidden_dim))\n",
        "            layers.append(nn.GELU())\n",
        "\n",
        "        # Bottleneck layer\n",
        "        layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
        "\n",
        "        # Create the MLP\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "        # Last layer\n",
        "        self.last_layer = nn.Linear(bottleneck_dim, out_dim)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        x = self.last_layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformerWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper for Vision Transformer backbones\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name: str, img_size: int = 224, pretrained: bool = False):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=pretrained,\n",
        "            img_size=img_size,\n",
        "            num_classes=0  # Remove classification head\n",
        "        )\n",
        "\n",
        "        # Get feature dimension\n",
        "        if hasattr(self.backbone, 'num_features'):\n",
        "            self.feature_dim = self.backbone.num_features\n",
        "        else:\n",
        "            # For ViT models, use embed_dim\n",
        "            self.feature_dim = self.backbone.embed_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "\n",
        "class DINOStudent(nn.Module):\n",
        "    \"\"\"\n",
        "    Student model for DINO framework\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone: nn.Module,\n",
        "        head: nn.Module,\n",
        "        momentum_teacher: float = 0.996,\n",
        "        use_momentum_schedule: bool = True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.head = head\n",
        "        self.momentum_teacher = momentum_teacher\n",
        "        self.use_momentum_schedule = use_momentum_schedule\n",
        "\n",
        "        # Initialize teacher with the same architecture but no gradients\n",
        "        self.teacher = None\n",
        "        self.init_teacher()\n",
        "\n",
        "        # Register buffer for momentum schedule\n",
        "        self.register_buffer('iteration', torch.tensor(0, dtype=torch.long))\n",
        "\n",
        "    def init_teacher(self):\n",
        "        \"\"\"Initialize teacher model with student weights\"\"\"\n",
        "        # Create a copy of student backbone and head\n",
        "        teacher_backbone = copy.deepcopy(self.backbone)\n",
        "        teacher_head = copy.deepcopy(self.head)\n",
        "\n",
        "        # Freeze teacher parameters\n",
        "        for param in teacher_backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in teacher_head.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.teacher = nn.ModuleDict({\n",
        "            'backbone': teacher_backbone,\n",
        "            'head': teacher_head\n",
        "        })\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_teacher(self):\n",
        "        \"\"\"Update teacher weights with exponential moving average\"\"\"\n",
        "        # Calculate momentum based on schedule if enabled\n",
        "        if self.use_momentum_schedule:\n",
        "            momentum = self.momentum_schedule()\n",
        "        else:\n",
        "            momentum = self.momentum_teacher\n",
        "\n",
        "        # Update teacher backbone\n",
        "        for param_s, param_t in zip(self.backbone.parameters(),\n",
        "                                   self.teacher['backbone'].parameters()):\n",
        "            param_t.data.mul_(momentum).add_((1 - momentum) * param_s.detach().data)\n",
        "\n",
        "        # Update teacher head\n",
        "        for param_s, param_t in zip(self.head.parameters(),\n",
        "                                   self.teacher['head'].parameters()):\n",
        "            param_t.data.mul_(momentum).add_((1 - momentum) * param_s.detach().data)\n",
        "\n",
        "        # Increment iteration counter\n",
        "        self.iteration += 1\n",
        "\n",
        "    def momentum_schedule(self):\n",
        "        \"\"\"Cosine momentum schedule from 0.996 to 1.0\"\"\"\n",
        "        # Original DINO schedule: cosine from base_m to 1.0\n",
        "        base_m = self.momentum_teacher\n",
        "        final_m = 1.0\n",
        "        # In practice, DINO uses a different schedule but this is a common approximation\n",
        "        return final_m - (final_m - base_m) * (torch.cos(torch.pi * self.iteration / 200000) + 1) / 2\n",
        "\n",
        "    def forward(self, x: torch.Tensor, return_features: bool = False):\n",
        "        \"\"\"Forward pass through student network\"\"\"\n",
        "        features = self.backbone(x)\n",
        "        output = self.head(features)\n",
        "\n",
        "        if return_features:\n",
        "            return output, features\n",
        "        return output\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def teacher_forward(self, x: torch.Tensor):\n",
        "        \"\"\"Forward pass through teacher network (no gradients)\"\"\"\n",
        "        features = self.teacher['backbone'](x)\n",
        "        output = self.teacher['head'](features)\n",
        "        return output\n",
        "\n",
        "\n",
        "class DINOTeacher(nn.Module):\n",
        "    \"\"\"\n",
        "    Teacher model for DINO framework (separate implementation for clarity)\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone: nn.Module, head: nn.Module):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.head = head\n",
        "\n",
        "        # Freeze all parameters\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"Forward pass through teacher network\"\"\"\n",
        "        with torch.no_grad():\n",
        "            features = self.backbone(x)\n",
        "            output = self.head(features)\n",
        "        return output\n",
        "\n",
        "\n",
        "class MultiCropDINO(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper for DINO that handles multiple crops\n",
        "    \"\"\"\n",
        "    def __init__(self, student: DINOStudent):\n",
        "        super().__init__()\n",
        "        self.student = student\n",
        "\n",
        "    def forward(self, x: List[torch.Tensor]):\n",
        "        \"\"\"\n",
        "        Forward pass for multiple crops\n",
        "\n",
        "        Args:\n",
        "            x: List of tensors representing different crops\n",
        "\n",
        "        Returns:\n",
        "            student_outputs: List of student outputs for each crop\n",
        "            teacher_outputs: List of teacher outputs for each crop\n",
        "        \"\"\"\n",
        "        student_outputs = []\n",
        "        teacher_outputs = []\n",
        "\n",
        "        # Process each crop through student and teacher\n",
        "        for crop in x:\n",
        "            # Student forward\n",
        "            student_out = self.student(crop)\n",
        "            student_outputs.append(student_out)\n",
        "\n",
        "            # Teacher forward\n",
        "            with torch.no_grad():\n",
        "                teacher_out = self.student.teacher_forward(crop)\n",
        "                teacher_outputs.append(teacher_out)\n",
        "\n",
        "        return student_outputs, teacher_outputs\n",
        "\n",
        "\n",
        "# Example usage and testing\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    backbone_name = \"vit_small_patch16_224\"\n",
        "    img_size = 224\n",
        "    in_dim = 384  # ViT-small feature dimension\n",
        "    out_dim = 65536  # As in original DINO paper\n",
        "    hidden_dim = 2048\n",
        "    bottleneck_dim = 256\n",
        "\n",
        "    # Create backbone\n",
        "    backbone = VisionTransformerWrapper(backbone_name, img_size, pretrained=False)\n",
        "\n",
        "    # Create DINO head\n",
        "    dino_head = DINO_MLP_HD(\n",
        "        in_dim=in_dim,\n",
        "        out_dim=out_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        bottleneck_dim=bottleneck_dim,\n",
        "        n_layers=4,\n",
        "        use_layer_norm=True\n",
        "    )\n",
        "\n",
        "    # Create student model\n",
        "    student = DINOStudent(\n",
        "        backbone=backbone,\n",
        "        head=dino_head,\n",
        "        momentum_teacher=0.996,\n",
        "        use_momentum_schedule=True\n",
        "    )\n",
        "\n",
        "    # Create multi-crop wrapper\n",
        "    model = MultiCropDINO(student)\n",
        "\n",
        "    # Print model information\n",
        "    print(\"Student Model:\")\n",
        "    print(student)\n",
        "    print(f\"Number of parameters: {sum(p.numel() for p in student.parameters()):,}\")\n",
        "    print(f\"Number of trainable parameters: {sum(p.numel() for p in student.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    print(\"\\nTeacher Model:\")\n",
        "    print(student.teacher)\n",
        "    print(f\"Number of parameters: {sum(p.numel() for p in student.teacher.parameters()):,}\")\n",
        "\n",
        "    # Test with sample input (2 global crops + 4 local crops)\n",
        "    batch_size = 2\n",
        "    global_crops = [torch.randn(batch_size, 3, img_size, img_size) for _ in range(2)]\n",
        "    local_crops = [torch.randn(batch_size, 3, img_size//2, img_size//2) for _ in range(4)]\n",
        "    all_crops = global_crops + local_crops\n",
        "\n",
        "    # Forward pass\n",
        "    student_outputs, teacher_outputs = model(all_crops)\n",
        "\n",
        "    print(f\"\\nNumber of crops: {len(all_crops)}\")\n",
        "    print(f\"Student outputs: {len(student_outputs)} tensors\")\n",
        "    print(f\"Teacher outputs: {len(teacher_outputs)} tensors\")\n",
        "\n",
        "    for i, (s_out, t_out) in enumerate(zip(student_outputs, teacher_outputs)):\n",
        "        print(f\"Crop {i}: student shape {s_out.shape}, teacher shape {t_out.shape}\")\n",
        "\n",
        "    # Update teacher\n",
        "    student.update_teacher()\n",
        "    print(f\"\\nUpdated teacher (iteration {student.iteration.item()})\")\n",
        "\n",
        "    # Test with a single image\n",
        "    single_image = torch.randn(1, 3, img_size, img_size)\n",
        "    student_output = student(single_image)\n",
        "    teacher_output = student.teacher_forward(single_image)\n",
        "\n",
        "    print(f\"\\nSingle image:\")\n",
        "    print(f\"Student output shape: {student_output.shape}\")\n",
        "    print(f\"Teacher output shape: {teacher_output.shape}\")"
      ],
      "metadata": {
        "id": "6gg7Ar8y6dUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DINO_MLP_HD(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hidden_dim=2048, bottleneck_dim=1256,\n",
        "                 n_layers=4, use_layer_norm=True,):\n",
        "        super().__init__()\n",
        "        # Build the MLP layers\n",
        "        layers = []\n",
        "\n",
        "        # Input layer\n",
        "        layers.append(nn.Linear(in_dim, hidden_dim))\n",
        "        if use_bn:\n",
        "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "        layers.append(nn.GELU())\n",
        "\n",
        "        # Hidden layers\n",
        "        for _ in range(n_layers - 2):\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            if use_layer_norm:\n",
        "                layers.append(nn.LayerNorm(hidden_dim))\n",
        "            layers.append(nn.GELU())\n",
        "\n",
        "        # Bottleneck layer\n",
        "        layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
        "\n",
        "        # Create the MLP\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "        self.last_layer=nn.linear(bottleneck_dim,out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        x=self.last_layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class VisionTransformerWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper for Vision Transformer backbones\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name: str, img_size: int = 224, pretrained: bool = True, is_teacher=True):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=pretrained,\n",
        "            img_size=img_size,\n",
        "            #num_classes=0  # Remove classification head\n",
        "        )\n",
        "\n",
        "        # Get feature dimension\n",
        "        if hasattr(self.backbone, 'num_features'):\n",
        "            self.feature_dim = self.backbone.num_features\n",
        "        else:\n",
        "            # For ViT models, use embed_dim\n",
        "            self.feature_dim = self.backbone.embed_dim\n",
        "        input_vec_dim=self.backbone.in_features\n",
        "        # self.backbone.head = nn.Identity()\n",
        "        layers = list(self.backbone.children())[:-1]\n",
        "        self.backbone = nn.Sequential(*layers)\n",
        "        for param in self.model.parameters():\n",
        "          if is_teacher:\n",
        "            param.requires_grad = False\n",
        "          else:\n",
        "            param.requires_grad = True\n",
        "        self.dino_mlp_head = DINO_MLP_HD(input_dim, out_dim)\n",
        "        dino_head = DINO_MLP_HD(\n",
        "        in_dim=input_vec_dim,\n",
        "        out_dim=1024,\n",
        "        hidden_dim=2038,\n",
        "        bottleneck_dim=152,\n",
        "        n_layers=5,\n",
        "        use_layer_norm=True\n",
        "    )\n",
        "\n",
        "    def forward(self, x):\n",
        "        vis_features=self.backbone(x)\n",
        "        cls_token=vis_features[:,0]\n",
        "        print(cls_token.shape, \"log class token shape\")\n",
        "        x=self.dino_mlp_head(cls_token)\n",
        "        return F.normalize(x, dim=-1, p=2)"
      ],
      "metadata": {
        "id": "kGKakIaH8Irp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DINOLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    DINO loss function implementation.\n",
        "\n",
        "    This loss function implements the self-distillation with no labels approach\n",
        "    used in the DINO paper. It consists of:\n",
        "    1. Cross-entropy loss between student and teacher outputs\n",
        "    2. Centering of teacher outputs to avoid collapse\n",
        "    3. Sharpening of teacher distributions with temperature\n",
        "\n",
        "    Args:\n",
        "        out_dim (int): Output dimension of the projection head\n",
        "        warmup_teacher_temp (float): Initial teacher temperature\n",
        "        teacher_temp (float): Final teacher temperature (after warmup)\n",
        "        warmup_teacher_temp_epochs (int): Number of warmup epochs for teacher temperature\n",
        "        nepochs (int): Total number of epochs\n",
        "        student_temp (float): Student temperature\n",
        "        center_momentum (float): Momentum for center update\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, out_dim, warmup_teacher_temp=0.04, teacher_temp=0.04,\n",
        "                 warmup_teacher_temp_epochs=30, nepochs=100, student_temp=0.1,\n",
        "                 center_momentum=0.9):\n",
        "        super().__init__()\n",
        "        self.student_temp = student_temp\n",
        "        self.center_momentum = center_momentum\n",
        "        self.register_buffer(\"center\", torch.zeros(1, out_dim))\n",
        "\n",
        "        # Temperature scheduling\n",
        "        self.teacher_temp_schedule = torch.cat((\n",
        "            torch.linspace(warmup_teacher_temp, teacher_temp, warmup_teacher_temp_epochs),\n",
        "            torch.ones(nepochs - warmup_teacher_temp_epochs) * teacher_temp\n",
        "        ))\n",
        "\n",
        "    def forward(self, student_output, teacher_output, epoch):\n",
        "        \"\"\"\n",
        "        Forward pass of the DINO loss.\n",
        "\n",
        "        Args:\n",
        "            student_output: List of student outputs for different crops\n",
        "            teacher_output: List of teacher outputs for different crops\n",
        "            epoch: Current epoch number (for temperature scheduling)\n",
        "\n",
        "        Returns:\n",
        "            Loss value\n",
        "        \"\"\"\n",
        "        # Get current teacher temperature\n",
        "        teacher_temp = self.teacher_temp_schedule[epoch].item()\n",
        "\n",
        "        # Gather all outputs\n",
        "        student_out = self.gather_outputs(student_output)\n",
        "        teacher_out = self.gather_outputs(teacher_output)\n",
        "\n",
        "        # Apply temperature to student outputs\n",
        "        student_out = student_out / self.student_temp\n",
        "\n",
        "        # Apply temperature and center to teacher outputs\n",
        "        teacher_out = F.softmax((teacher_out - self.center) / teacher_temp, dim=-1)\n",
        "        teacher_out = teacher_out.detach()  # Detach to stop gradients\n",
        "\n",
        "        # Calculate cross-entropy loss\n",
        "        loss = -torch.sum(teacher_out * F.log_softmax(student_out, dim=-1), dim=-1)\n",
        "        loss = loss.mean()\n",
        "\n",
        "        # Update center\n",
        "        self.update_center(teacher_output)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def gather_outputs(self, outputs):\n",
        "        \"\"\"\n",
        "        Gather outputs from all crops and concatenate them.\n",
        "        \"\"\"\n",
        "        return torch.cat([output for output in outputs], dim=0)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_center(self, teacher_output):\n",
        "        \"\"\"\n",
        "        Update center used for teacher output centering.\n",
        "        \"\"\"\n",
        "        # Gather all teacher outputs\n",
        "        teacher_out = self.gather_outputs(teacher_output)\n",
        "\n",
        "        # Calculate batch mean\n",
        "        batch_center = torch.mean(teacher_out, dim=0, keepdim=True)\n",
        "\n",
        "        # Update center with momentum\n",
        "        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)\n"
      ],
      "metadata": {
        "id": "q4ECHAIXEsYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test parameters\n",
        "model_name = \"vit_tiny_patch16_224\"\n",
        "img_size = 224\n",
        "batch_size = 2\n",
        "\n",
        "print(\"Testing VisionTransformerWrapper...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create student model (not teacher)\n",
        "print(\"Creating student model...\")\n",
        "student_model = VisionTransformerWrapper(\n",
        "    model_name=model_name,\n",
        "    img_size=img_size,\n",
        "    pretrained=False,  # Use False for faster testing\n",
        "    is_teacher=False\n",
        ")\n",
        "\n",
        "# Create teacher model\n",
        "print(\"Creating teacher model...\")\n",
        "teacher_model = VisionTransformerWrapper(\n",
        "    model_name=model_name,\n",
        "    img_size=img_size,\n",
        "    pretrained=False,  # Use False for faster testing\n",
        "    is_teacher=True\n",
        ")"
      ],
      "metadata": {
        "id": "ssi6redXb1GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "import timm\n",
        "\n",
        "# First, let's define the DINO_MLP_HD class if it's not already defined\n",
        "class DINO_MLP_HD(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hidden_dim=2048, bottleneck_dim=256,\n",
        "                 n_layers=4, use_layer_norm=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Build the MLP layers\n",
        "        layers = []\n",
        "\n",
        "        # Input layer\n",
        "        layers.append(nn.Linear(in_dim, hidden_dim))\n",
        "        if use_layer_norm:\n",
        "            layers.append(nn.LayerNorm(hidden_dim))\n",
        "        layers.append(nn.GELU())\n",
        "\n",
        "        # Hidden layers\n",
        "        for _ in range(n_layers - 2):\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            if use_layer_norm:\n",
        "                layers.append(nn.LayerNorm(hidden_dim))\n",
        "            layers.append(nn.GELU())\n",
        "\n",
        "        # Bottleneck layer\n",
        "        layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
        "\n",
        "        # Create the MLP\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "        # Last layer\n",
        "        self.last_layer = nn.Linear(bottleneck_dim, out_dim)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        x = self.last_layer(x)\n",
        "        return x\n",
        "\n",
        "# Now let's define the VisionTransformerWrapper with a fix for the parameter freezing\n",
        "class VisionTransformerWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper for Vision Transformer backbones\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name: str, img_size: int = 224, pretrained: bool = True, is_teacher=True):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=pretrained,\n",
        "            img_size=img_size,\n",
        "            #num_classes=0  # Remove classification head\n",
        "        )\n",
        "\n",
        "        # Get feature dimension\n",
        "        if hasattr(self.backbone, 'num_features'):\n",
        "            self.feature_dim = self.backbone.num_features\n",
        "        else:\n",
        "            # For ViT models, use embed_dim\n",
        "            self.feature_dim = self.backbone.embed_dim\n",
        "\n",
        "        # Get input dimension from the backbone\n",
        "        input_vec_dim = self.feature_dim\n",
        "\n",
        "        # Remove the classification head\n",
        "        layers = list(self.backbone.children())[:-1]\n",
        "        self.backbone = nn.Sequential(*layers)\n",
        "\n",
        "        # Freeze parameters if it's a teacher\n",
        "        for param in self.backbone.parameters():\n",
        "            if is_teacher:\n",
        "                param.requires_grad = False\n",
        "            else:\n",
        "                param.requires_grad = True\n",
        "\n",
        "        # Create the DINO head\n",
        "        self.dino_mlp_head = DINO_MLP_HD(\n",
        "            in_dim=input_vec_dim,\n",
        "            out_dim=1024,\n",
        "            hidden_dim=2048,\n",
        "            bottleneck_dim=256,\n",
        "            n_layers=3,\n",
        "            use_layer_norm=True\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        vis_features = self.backbone(x)\n",
        "        # Assuming the first token is the class token\n",
        "        cls_token = vis_features[:, 0]\n",
        "        print(f\"Class token shape: {cls_token.shape}\")\n",
        "        x = self.dino_mlp_head(cls_token)\n",
        "        return F.normalize(x, dim=-1, p=2)\n",
        "\n",
        "# Test function\n",
        "def test_vision_transformer_wrapper():\n",
        "    # Test parameters\n",
        "    model_name = \"vit_tiny_patch16_224\"\n",
        "    img_size = 224\n",
        "    batch_size = 2\n",
        "\n",
        "    print(\"Testing VisionTransformerWrapper...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create student model (not teacher)\n",
        "    print(\"Creating student model...\")\n",
        "    student_model = VisionTransformerWrapper(\n",
        "        model_name=model_name,\n",
        "        img_size=img_size,\n",
        "        pretrained=False,  # Use False for faster testing\n",
        "        is_teacher=False\n",
        "    )\n",
        "\n",
        "    # Create teacher model\n",
        "    print(\"Creating teacher model...\")\n",
        "    teacher_model = VisionTransformerWrapper(\n",
        "        model_name=model_name,\n",
        "        img_size=img_size,\n",
        "        pretrained=False,  # Use False for faster testing\n",
        "        is_teacher=True\n",
        "    )\n",
        "\n",
        "    # Create a dummy input tensor\n",
        "    dummy_input = torch.randn(batch_size, 3, img_size, img_size)\n",
        "    print(f\"Input shape: {dummy_input.shape}\")\n",
        "\n",
        "    # Test student model\n",
        "    print(\"\\nTesting student model forward pass...\")\n",
        "    student_output = student_model(dummy_input)\n",
        "    print(f\"Student output shape: {student_output.shape}\")\n",
        "    print(f\"Student output norm: {torch.norm(student_output, dim=1)}\")\n",
        "\n",
        "    # Check gradient requirements for student\n",
        "    student_params_require_grad = sum(p.requires_grad for p in student_model.parameters())\n",
        "    print(f\"Student parameters requiring grad: {student_params_require_grad}\")\n",
        "\n",
        "    # Test teacher model\n",
        "    print(\"\\nTesting teacher model forward pass...\")\n",
        "    teacher_output = teacher_model(dummy_input)\n",
        "    print(f\"Teacher output shape: {teacher_output.shape}\")\n",
        "    print(f\"Teacher output norm: {torch.norm(teacher_output, dim=1)}\")\n",
        "\n",
        "    # Check gradient requirements for teacher\n",
        "    teacher_params_require_grad = sum(p.requires_grad for p in teacher_model.parameters())\n",
        "    print(f\"Teacher parameters requiring grad: {teacher_params_require_grad}\")\n",
        "\n",
        "    # Test with gradients\n",
        "    print(\"\\nTesting backward pass with student model...\")\n",
        "    dummy_input.requires_grad = True\n",
        "    student_output = student_model(dummy_input)\n",
        "\n",
        "    # Create a dummy loss\n",
        "    dummy_target = torch.randn_like(student_output)\n",
        "    loss = F.mse_loss(student_output, dummy_target)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    print(f\"Loss: {loss.item()}\")\n",
        "    print(\"Backward pass completed successfully!\")\n",
        "\n",
        "    # Verify gradients\n",
        "    has_gradients = any(p.grad is not None for p in student_model.parameters() if p.requires_grad)\n",
        "    print(f\"Model parameters have gradients: {has_gradients}\")\n",
        "\n",
        "    # Print model summary\n",
        "    print(\"\\nModel Summary:\")\n",
        "    print(\"=\" * 50)\n",
        "    total_params = sum(p.numel() for p in student_model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in student_model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
        "\n",
        "    # Test with different input sizes\n",
        "    print(\"\\nTesting with different input sizes...\")\n",
        "    for test_size in [224]:\n",
        "        test_input = torch.randn(batch_size, 3, test_size, test_size)\n",
        "        try:\n",
        "            output = student_model(test_input)\n",
        "            print(f\"Input size {test_size}x{test_size}: Output shape {output.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Input size {test_size}x{test_size}: Error - {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_vision_transformer_wrapper()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqJH9LqmaD1m",
        "outputId": "311fe6bf-12c0-48f3-e8aa-240a9c2a4f10"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing VisionTransformerWrapper...\n",
            "==================================================\n",
            "Creating student model...\n",
            "Creating teacher model...\n",
            "Input shape: torch.Size([2, 3, 224, 224])\n",
            "\n",
            "Testing student model forward pass...\n",
            "Class token shape: torch.Size([2, 192])\n",
            "Student output shape: torch.Size([2, 1024])\n",
            "Student output norm: tensor([1.0000, 1.0000], grad_fn=<LinalgVectorNormBackward0>)\n",
            "Student parameters requiring grad: 160\n",
            "\n",
            "Testing teacher model forward pass...\n",
            "Class token shape: torch.Size([2, 192])\n",
            "Teacher output shape: torch.Size([2, 1024])\n",
            "Teacher output norm: tensor([1.0000, 1.0000], grad_fn=<LinalgVectorNormBackward0>)\n",
            "Teacher parameters requiring grad: 12\n",
            "\n",
            "Testing backward pass with student model...\n",
            "Class token shape: torch.Size([2, 192])\n",
            "Loss: 0.9629690051078796\n",
            "Backward pass completed successfully!\n",
            "Model parameters have gradients: True\n",
            "\n",
            "Model Summary:\n",
            "==================================================\n",
            "Total parameters: 10,873,920\n",
            "Trainable parameters: 10,873,920\n",
            "Frozen parameters: 0\n",
            "\n",
            "Testing with different input sizes...\n",
            "Class token shape: torch.Size([2, 192])\n",
            "Input size 224x224: Output shape torch.Size([2, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "```markdown\n",
        "# Setup Instructions\n",
        "\n",
        "## 1. Clone the Repository\n",
        "```bash\n",
        "git clone https://github.com/basaanithanaveenkumar/object-detection-BBD.git\n",
        "cd object-detection-BBD\n",
        "```\n",
        "\n",
        "## 2. Create Data Directory\n",
        "```bash\n",
        "mkdir -p data\n",
        "```\n",
        "\n",
        "## 3. Download Dataset\n",
        "```bash\n",
        "python scripts/download_dataset.py\n",
        "```\n",
        "\n",
        "## 4. Organize Directory Structure\n",
        "```bash\n",
        "mv data/100k/val data/100k/valid\n",
        "```\n",
        "\n",
        "## 5. Convert to COCO Format\n",
        "```bash\n",
        "python scripts/convert_to_coco.py\n",
        "```\n",
        "\n",
        "## Workflow Summary\n",
        "This setup process:\n",
        "1. Clones the object detection project repository\n",
        "2. Creates the necessary directory structure\n",
        "3. Downloads the BBD (Berkeley DeepDrive) dataset\n",
        "4. Renames the validation directory to match expected conventions\n",
        "5. Converts the BBD dataset format to standard COCO format for compatibility with object detection frameworks\n",
        "```"
      ],
      "metadata": {
        "id": "v2xVDUq7PR-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the object detection-BBD project\n",
        "!git clone https://github.com/basaanithanaveenkumar/object-detection-BBD.git\n",
        "%cd object-detection-BBD\n",
        "\n",
        "# create a dir to store the dataset\n",
        "!mkdir -p data\n",
        "\n",
        "# Download and extract dataset\n",
        "!python scripts/download_dataset.py\n",
        "\n",
        "# organise the dir structure for coco datset\n",
        "!mv data/100k/val data/100k/valid\n",
        "\n",
        "# convert the BBD dataset into COCO Dataset\n",
        "!python scripts/convert_to_coco.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW81hTcznzSk",
        "outputId": "82de79d1-e1e7-44d7-aa7e-2f7d6aa76dd0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'object-detection-BBD'...\n",
            "remote: Enumerating objects: 176, done.\u001b[K\n",
            "remote: Counting objects: 100% (176/176), done.\u001b[K\n",
            "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
            "remote: Total 176 (delta 111), reused 92 (delta 46), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (176/176), 5.90 MiB | 30.80 MiB/s, done.\n",
            "Resolving deltas: 100% (111/111), done.\n",
            "/content/object-detection-BBD\n",
            "\n",
            "Downloading images_100k...\n",
            "data/bdd100k_images_100k.zip: 100% 5.28G/5.28G [11:09<00:00, 8.47MiB/s]\n",
            "Extracting data/bdd100k_images_100k.zip...\n",
            "Extracted to data\n",
            "\n",
            "Downloading labels...\n",
            "data/bdd100k_labels.zip: 100% 181M/181M [00:17<00:00, 10.8MiB/s]\n",
            "Extracting data/bdd100k_labels.zip...\n",
            "Extracted to data\n",
            "\n",
            "BDD100K dataset download complete!\n",
            "{'car': 1, 'traffic sign': 2, 'traffic light': 3, 'person': 4, 'truck': 5, 'bus': 6, 'bike': 7, 'rider': 8, 'motor': 9, 'train': 10}\n",
            "Processing annotations: 100% 10000/10000 [00:03<00:00, 3027.12it/s]\n",
            "Conversion complete. COCO format JSON saved to data/100k/valid/_annotations.coco.json\n",
            "{'car': 1, 'traffic sign': 2, 'traffic light': 3, 'person': 4, 'truck': 5, 'bus': 6, 'bike': 7, 'rider': 8, 'motor': 9, 'train': 10}\n",
            "Processing annotations: 100% 70000/70000 [00:22<00:00, 3085.50it/s]\n",
            "Conversion complete. COCO format JSON saved to data/100k/train/_annotations.coco.json\n",
            "{'car': 1, 'traffic sign': 2, 'traffic light': 3, 'person': 4, 'truck': 5, 'bus': 6, 'bike': 7, 'rider': 8, 'motor': 9, 'train': 10}\n",
            "Processing annotations: 100% 20000/20000 [00:06<00:00, 3080.95it/s]\n",
            "Conversion complete. COCO format JSON saved to data/100k/test/_annotations.coco.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DINOLoss(nn.Module):\n",
        "    def __init__(self, out_dim, ncrops, warmup_teacher_temp, teacher_temp,\n",
        "                 warmup_teacher_temp_epochs, nepochs, student_temp=0.1,\n",
        "                 center_momentum=0.9):\n",
        "        super().__init__()\n",
        "        self.student_temp = student_temp\n",
        "        self.center_momentum = center_momentum\n",
        "        self.ncrops = ncrops\n",
        "        self.register_buffer(\"center\", torch.zeros(1, out_dim))\n",
        "        # we apply a warm up for the teacher temperature because\n",
        "        # a too high temperature makes the training instable at the beginning\n",
        "        self.teacher_temp_schedule = np.concatenate((\n",
        "            np.linspace(warmup_teacher_temp,\n",
        "                        teacher_temp, warmup_teacher_temp_epochs),\n",
        "            np.ones(nepochs - warmup_teacher_temp_epochs) * teacher_temp\n",
        "        ))\n",
        "\n",
        "    def forward(self, student_output, teacher_output, epoch):\n",
        "        \"\"\"\n",
        "        Cross-entropy between softmax outputs of the teacher and student networks.\n",
        "        \"\"\"\n",
        "        student_out = student_output / self.student_temp\n",
        "        student_out = student_out.chunk(self.ncrops)\n",
        "\n",
        "        # teacher centering and sharpening\n",
        "        temp = self.teacher_temp_schedule[epoch]\n",
        "        teacher_out = F.softmax((teacher_output - self.center) / temp, dim=-1)\n",
        "        teacher_out = teacher_out.detach().chunk(2)\n",
        "\n",
        "        total_loss = 0\n",
        "        n_loss_terms = 0\n",
        "        for iq, q in enumerate(teacher_out):\n",
        "            for v in range(len(student_out)):\n",
        "                if v == iq:\n",
        "                    # we skip cases where student and teacher operate on the same view\n",
        "                    continue\n",
        "                loss = torch.sum(-q * F.log_softmax(student_out[v], dim=-1), dim=-1)\n",
        "                total_loss += loss.mean()\n",
        "                n_loss_terms += 1\n",
        "        total_loss /= n_loss_terms\n",
        "        self.update_center(teacher_output)\n",
        "        return total_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_center(self, teacher_output):\n",
        "        \"\"\"\n",
        "        Update center used for teacher output.\n",
        "        \"\"\"\n",
        "        batch_center = torch.sum(teacher_output, dim=0, keepdim=True)\n",
        "        dist.all_reduce(batch_center)\n",
        "        batch_center = batch_center / (len(teacher_output) * dist.get_world_size())\n",
        "\n",
        "        # ema update\n",
        "        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)\n",
        "\n",
        "\n",
        "\n",
        "# # one more generated by deep seek\n",
        "\n",
        "\n",
        "class DINOLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    DINO loss function implementation.\n",
        "\n",
        "    This loss function implements the self-distillation with no labels approach\n",
        "    used in the DINO paper. It consists of:\n",
        "    1. Cross-entropy loss between student and teacher outputs\n",
        "    2. Centering of teacher outputs to avoid collapse\n",
        "    3. Sharpening of teacher distributions with temperature\n",
        "\n",
        "    Args:\n",
        "        out_dim (int): Output dimension of the projection head\n",
        "        warmup_teacher_temp (float): Initial teacher temperature\n",
        "        teacher_temp (float): Final teacher temperature (after warmup)\n",
        "        warmup_teacher_temp_epochs (int): Number of warmup epochs for teacher temperature\n",
        "        nepochs (int): Total number of epochs\n",
        "        student_temp (float): Student temperature\n",
        "        center_momentum (float): Momentum for center update\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, out_dim, warmup_teacher_temp=0.04, teacher_temp=0.07,\n",
        "                 warmup_teacher_temp_epochs=30, nepochs=100, student_temp=0.1,\n",
        "                 center_momentum=0.9):\n",
        "        super().__init__()\n",
        "        self.student_temp = student_temp\n",
        "        self.center_momentum = center_momentum\n",
        "        self.nepochs = nepochs\n",
        "        self.warmup_teacher_temp_epochs = warmup_teacher_temp_epochs\n",
        "\n",
        "        # Register buffer for center\n",
        "        self.register_buffer(\"center\", torch.zeros(1, out_dim))\n",
        "\n",
        "        # Temperature scheduling\n",
        "        self.warmup_teacher_temp = warmup_teacher_temp\n",
        "        self.teacher_temp = teacher_temp\n",
        "\n",
        "    def forward(self, student_output, teacher_output, epoch):\n",
        "        \"\"\"\n",
        "        Forward pass of the DINO loss.\n",
        "\n",
        "        Args:\n",
        "            student_output: List of student outputs for different crops\n",
        "            teacher_output: List of teacher outputs for different crops\n",
        "            epoch: Current epoch number (for temperature scheduling)\n",
        "\n",
        "        Returns:\n",
        "            Loss value\n",
        "        \"\"\"\n",
        "        # Get current teacher temperature (with warmup)\n",
        "        teacher_temp = self.get_teacher_temp(epoch)\n",
        "\n",
        "        # Gather all outputs\n",
        "        student_out = self.gather_outputs(student_output)\n",
        "        teacher_out = self.gather_outputs(teacher_output).detach()  # Detach early\n",
        "\n",
        "        # Apply temperature to student outputs\n",
        "        if isinstance(student_output, list):\n",
        "            student_output = torch.cat(student_output, dim=0)\n",
        "            teacher_output = torch.cat(teacher_output, dim=0)\n",
        "        student_out = student_out / self.student_temp\n",
        "\n",
        "        # Apply temperature and center to teacher outputs\n",
        "        teacher_out = (teacher_out - self.center) / teacher_temp\n",
        "        teacher_out = F.softmax(teacher_out, dim=-1)\n",
        "\n",
        "        # Calculate cross-entropy loss\n",
        "        loss = -torch.sum(teacher_out * F.log_softmax(student_out, dim=-1), dim=-1)\n",
        "        loss = loss.mean()\n",
        "\n",
        "        # Update center\n",
        "        self.update_center(teacher_output)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def get_teacher_temp(self, epoch):\n",
        "        \"\"\"Get teacher temperature with warmup schedule\"\"\"\n",
        "        if epoch < self.warmup_teacher_temp_epochs:\n",
        "            # Linear warmup\n",
        "            return self.warmup_teacher_temp + (self.teacher_temp - self.warmup_teacher_temp) * \\\n",
        "                   epoch / self.warmup_teacher_temp_epochs\n",
        "        else:\n",
        "            return self.teacher_temp\n",
        "\n",
        "    def gather_outputs(self, outputs):\n",
        "        \"\"\"\n",
        "        Gather outputs from all crops and concatenate them.\n",
        "        \"\"\"\n",
        "        return torch.cat([output for output in outputs], dim=0)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_center(self, teacher_output):\n",
        "        \"\"\"\n",
        "        Update center used for teacher output centering.\n",
        "        \"\"\"\n",
        "        # Gather all teacher outputs (raw, before temperature/softmax)\n",
        "        teacher_out = self.gather_outputs(teacher_output)\n",
        "\n",
        "        # Calculate batch mean\n",
        "        batch_center = torch.mean(teacher_out, dim=0, keepdim=True)\n",
        "\n",
        "        # Update center with momentum\n",
        "        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)\n"
      ],
      "metadata": {
        "id": "nG5VBIe0FDtJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pycocotools.coco import COCO\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "import random\n",
        "\n",
        "class COCOMultiCropDataset(Dataset):\n",
        "    \"\"\"\n",
        "    COCO dataset with multi-crop transformations for DINO training\n",
        "    \"\"\"\n",
        "    def __init__(self, annFile, dataDir, global_crop_size=224, local_crop_size=96,\n",
        "                 num_local_crops=4, transform=None):\n",
        "        self.coco = COCO(annFile)\n",
        "        self.dataDir = dataDir\n",
        "        self.img_ids = self.coco.getImgIds()\n",
        "        self.global_crop_size = global_crop_size\n",
        "        self.local_crop_size = local_crop_size\n",
        "        self.num_local_crops = num_local_crops\n",
        "\n",
        "        # Normalization (ImageNet stats)\n",
        "        self.normalize = transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "\n",
        "        # Default transformations if none provided\n",
        "        if transform is None:\n",
        "            self.transform = self.get_default_transforms()\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "    def get_default_transforms(self):\n",
        "        \"\"\"\n",
        "        Default DINO multi-crop transformations\n",
        "        \"\"\"\n",
        "        # Global crops (2x)\n",
        "        global_transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(self.global_crop_size, scale=(0.4, 1.0)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomApply([\n",
        "                transforms.ColorJitter(0.4, 0.4, 0.2, 0.1)\n",
        "            ], p=0.8),\n",
        "            transforms.RandomGrayscale(p=0.2),\n",
        "            transforms.RandomApply([transforms.GaussianBlur(5)], p=0.5),\n",
        "            transforms.ToTensor(),\n",
        "            self.normalize\n",
        "        ])\n",
        "\n",
        "        # Local crops (multiple)\n",
        "        local_transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(self.local_crop_size, scale=(0.05, 0.4)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.Resize((self.global_crop_size, self.global_crop_size)),\n",
        "            transforms.RandomApply([\n",
        "                transforms.ColorJitter(0.4, 0.4, 0.2, 0.1)\n",
        "            ], p=0.8),\n",
        "            transforms.RandomGrayscale(p=0.2),\n",
        "            transforms.RandomApply([transforms.GaussianBlur(5)], p=0.5),\n",
        "            transforms.ToTensor(),\n",
        "            self.normalize\n",
        "        ])\n",
        "\n",
        "        return {\n",
        "            'global': global_transform,\n",
        "            'local': local_transform\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.img_ids[idx]\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "\n",
        "        # Load image\n",
        "        img_path = f\"{self.dataDir}/{img_info['file_name']}\"\n",
        "        image = cv2.imread(img_path)\n",
        "\n",
        "        if image is None:\n",
        "            # If image loading fails, return a random image\n",
        "            image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Convert to PIL Image for transformations\n",
        "        image_pil = transforms.ToPILImage()(image)\n",
        "\n",
        "        # Apply transformations\n",
        "        crops = []\n",
        "\n",
        "        # Global crops (2x)\n",
        "        for _ in range(2):\n",
        "            crops.append(self.transform['global'](image_pil))\n",
        "\n",
        "        # Local crops (num_local_crops x)\n",
        "        for _ in range(self.num_local_crops):\n",
        "            crops.append(self.transform['local'](image_pil))\n",
        "\n",
        "        return crops\n",
        "\n",
        "# Create dataset and data loader\n",
        "def create_coco_dataloader(annFile, dataDir, batch_size=4, num_workers=4,\n",
        "                          global_crop_size=224, local_crop_size=96, num_local_crops=4):\n",
        "    \"\"\"\n",
        "    Create COCO data loader for DINO training\n",
        "    \"\"\"\n",
        "    dataset = COCOMultiCropDataset(\n",
        "        annFile=annFile,\n",
        "        dataDir=dataDir,\n",
        "        global_crop_size=global_crop_size,\n",
        "        local_crop_size=local_crop_size,\n",
        "        num_local_crops=num_local_crops\n",
        "    )\n",
        "\n",
        "    # Custom collate function for multi-crop data\n",
        "    def collate_fn(batch):\n",
        "        # batch is a list of lists of crops\n",
        "        # We need to transpose it to group crops by type\n",
        "        transposed = list(zip(*batch))\n",
        "        return [torch.stack(crops) for crops in transposed]\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_fn,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize COCO dataset\n",
        "    dataDir = \"/content/object-detection-BBD/data/100k/test/\"\n",
        "    annFile = f\"{dataDir}/_annotations.coco.json\"\n",
        "\n",
        "    # Create data loader\n",
        "    dataloader = create_coco_dataloader(\n",
        "        annFile=annFile,\n",
        "        dataDir=dataDir,\n",
        "        batch_size=4,\n",
        "        num_workers=4,\n",
        "        global_crop_size=224,\n",
        "        local_crop_size=96,\n",
        "        num_local_crops=4\n",
        "    )\n",
        "\n",
        "    # Test the data loader\n",
        "    for batch_idx, crops in enumerate(dataloader):\n",
        "        print(f\"Batch {batch_idx}:\")\n",
        "        for i, crop_batch in enumerate(crops):\n",
        "            print(f\"  Crop {i}: shape {crop_batch.shape}\")\n",
        "\n",
        "        if batch_idx >= 2:  # Just test a few batches\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyQLmDc2ogDM",
        "outputId": "456af277-95c3-4204-c3fc-6b042d428e7a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.57s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0:\n",
            "  Crop 0: shape torch.Size([4, 3, 224, 224])\n",
            "  Crop 1: shape torch.Size([4, 3, 224, 224])\n",
            "  Crop 2: shape torch.Size([4, 3, 224, 224])\n",
            "  Crop 3: shape torch.Size([4, 3, 224, 224])\n",
            "  Crop 4: shape torch.Size([4, 3, 224, 224])\n",
            "  Crop 5: shape torch.Size([4, 3, 224, 224])\n",
            "Batch 1:\n",
            "  Crop 0: shape torch.Size([4, 3, 224, 224])\n",
            "  Crop 1: shape torch.Size([4, 3, 224, 224])\n",
            "  Crop 2: shape torch.Size([4, 3, 224, 224])\n",
            "  Crop 3: shape torch.Size([4, 3, 224, 224])\n",
            "  Crop 4: shape torch.Size([4, 3, 224, 224])\n",
            "  Crop 5: shape torch.Size([4, 3, 224, 224])\n",
            "Batch 2:\n",
            "  Crop 0: shape torch.Size([4, 3, 224, 224])\n",
            "  Crop 1: shape torch.Size([4, 3, 224, 224])\n",
            "  Crop 2: shape torch.Size([4, 3, 224, 224])\n",
            "  Crop 3: shape torch.Size([4, 3, 224, 224])\n",
            "  Crop 4: shape torch.Size([4, 3, 224, 224])\n",
            "  Crop 5: shape torch.Size([4, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import numpy as np\n",
        "import time\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"DINO_Trainer\")\n",
        "\n",
        "class DINOTrainer:\n",
        "    def __init__(self, student_model, teacher_model, dataloader,\n",
        "                 loss_fn, optimizer, device, out_dir=\"./dino_checkpoints\",\n",
        "                 warmup_epochs=10, total_epochs=100, save_freq=10,\n",
        "                 use_amp=True, base_lr=0.0005):  # Added base_lr parameter\n",
        "        \"\"\"\n",
        "        DINO trainer for self-supervised learning\n",
        "\n",
        "        Args:\n",
        "            student_model: Student model (trainable)\n",
        "            teacher_model: Teacher model (EMA of student)\n",
        "            dataloader: DataLoader with multi-crop images\n",
        "            loss_fn: DINO loss function\n",
        "            optimizer: Optimizer for student model\n",
        "            device: Training device (cuda/cpu)\n",
        "            out_dir: Directory to save checkpoints\n",
        "            warmup_epochs: Number of warmup epochs for learning rate\n",
        "            total_epochs: Total training epochs\n",
        "            save_freq: Frequency of saving checkpoints\n",
        "            use_amp: Whether to use automatic mixed precision\n",
        "            base_lr: Base learning rate for scheduling\n",
        "        \"\"\"\n",
        "        self.student = student_model\n",
        "        self.teacher = teacher_model\n",
        "        self.dataloader = dataloader\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.out_dir = Path(out_dir)\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.total_epochs = total_epochs\n",
        "        self.save_freq = save_freq\n",
        "        self.use_amp = use_amp\n",
        "        self.base_lr = base_lr  # Store base learning rate\n",
        "\n",
        "        # Create output directory\n",
        "        self.out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        self.lr_schedule = self._get_lr_schedule()\n",
        "\n",
        "        # Mixed precision scaler\n",
        "        self.scaler = GradScaler(enabled=use_amp)\n",
        "\n",
        "        # Move models to device\n",
        "        self.student.to(device)\n",
        "        self.teacher.to(device)\n",
        "\n",
        "        # Set teacher to eval mode\n",
        "        self.teacher.eval()\n",
        "\n",
        "        # Training state\n",
        "        self.epoch = 0\n",
        "        self.global_step = 0\n",
        "        self.best_loss = float('inf')\n",
        "\n",
        "        # Logging\n",
        "        logger.info(f\"DINO Trainer initialized on device: {device}\")\n",
        "        logger.info(f\"Using mixed precision: {use_amp}\")\n",
        "        logger.info(f\"Base learning rate: {base_lr}\")\n",
        "\n",
        "    def _get_lr_schedule(self):\n",
        "        \"\"\"Create learning rate schedule with warmup\"\"\"\n",
        "        def lr_schedule(step):\n",
        "            # Warmup for the first warmup_steps\n",
        "            warmup_steps = self.warmup_epochs * len(self.dataloader)\n",
        "            if step < warmup_steps:\n",
        "                return (step + 1) / warmup_steps\n",
        "            else:\n",
        "                # Cosine decay after warmup\n",
        "                total_steps = self.total_epochs * len(self.dataloader)\n",
        "                return 0.5 * (1 + np.cos(np.pi * (step - warmup_steps) / (total_steps - warmup_steps)))\n",
        "\n",
        "        return lr_schedule\n",
        "\n",
        "    def update_teacher(self, momentum=0.996):\n",
        "        \"\"\"Update teacher model with EMA of student weights\"\"\"\n",
        "        with torch.no_grad():\n",
        "            for param_s, param_t in zip(self.student.parameters(), self.teacher.parameters()):\n",
        "                param_t.data.mul_(momentum).add_((1 - momentum) * param_s.data)\n",
        "\n",
        "    def train_epoch(self):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.student.train()\n",
        "\n",
        "        epoch_loss = 0\n",
        "        num_batches = len(self.dataloader)\n",
        "\n",
        "        for batch_idx, crops in enumerate(self.dataloader):\n",
        "            # Move crops to device\n",
        "            crops = [crop.to(self.device, non_blocking=True) for crop in crops]\n",
        "\n",
        "            # Update learning rate\n",
        "            self._adjust_learning_rate(self.global_step)\n",
        "            lr = self.optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "            # Forward pass\n",
        "            with autocast(enabled=self.use_amp):\n",
        "                # Student forward pass (all crops)\n",
        "                student_outputs = []\n",
        "                for crop in crops:\n",
        "                    student_outputs.append(self.student(crop))\n",
        "\n",
        "                # Teacher forward pass (only global crops)\n",
        "                teacher_outputs = []\n",
        "                with torch.no_grad():\n",
        "                    for crop in crops[:2]:  # First two are global crops\n",
        "                        teacher_outputs.append(self.teacher(crop))\n",
        "\n",
        "                # Compute loss\n",
        "                print(len(student_outputs),\"len of student outputs\")\n",
        "                print(len(teacher_outputs), \"len of teacher outputs\")\n",
        "                loss = self.loss_fn(student_outputs, teacher_outputs, self.epoch)\n",
        "\n",
        "            # Backward pass\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            if self.use_amp:\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            # Update teacher with EMA\n",
        "            self.update_teacher()\n",
        "\n",
        "            # Update metrics\n",
        "            epoch_loss += loss.item()\n",
        "            self.global_step += 1\n",
        "\n",
        "            # Log progress\n",
        "            if batch_idx % 100 == 0:\n",
        "                logger.info(\n",
        "                    f\"Epoch {self.epoch}/{self.total_epochs} | \"\n",
        "                    f\"Batch {batch_idx}/{num_batches} | \"\n",
        "                    f\"Loss: {loss.item():.4f} | \"\n",
        "                    f\"LR: {lr:.6f}\"\n",
        "                )\n",
        "\n",
        "        return epoch_loss / num_batches\n",
        "\n",
        "    def _adjust_learning_rate(self, step):\n",
        "        \"\"\"Adjust learning rate based on schedule\"\"\"\n",
        "        # Calculate the multiplier from the schedule\n",
        "        multiplier = self.lr_schedule(step)\n",
        "\n",
        "        # Set the learning rate for all parameter groups\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group[\"lr\"] = self.base_lr * multiplier\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        logger.info(\"Starting DINO training...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(self.epoch, self.total_epochs):\n",
        "            self.epoch = epoch\n",
        "\n",
        "            # Train for one epoch\n",
        "            epoch_loss = self.train_epoch()\n",
        "\n",
        "            # Log epoch results\n",
        "            logger.info(\n",
        "                f\"Epoch {epoch}/{self.total_epochs} | \"\n",
        "                f\"Avg Loss: {epoch_loss:.4f} | \"\n",
        "                f\"Time: {time.time() - start_time:.2f}s\"\n",
        "            )\n",
        "\n",
        "            # Save checkpoint\n",
        "            if epoch % self.save_freq == 0 or epoch == self.total_epochs - 1:\n",
        "                self.save_checkpoint(epoch_loss)\n",
        "\n",
        "            # Update best loss\n",
        "            if epoch_loss < self.best_loss:\n",
        "                self.best_loss = epoch_loss\n",
        "                self.save_checkpoint(epoch_loss, is_best=True)\n",
        "\n",
        "        logger.info(f\"Training completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    def save_checkpoint(self, loss, is_best=False):\n",
        "        \"\"\"Save training checkpoint\"\"\"\n",
        "        checkpoint = {\n",
        "            'epoch': self.epoch,\n",
        "            'global_step': self.global_step,\n",
        "            'student_state_dict': self.student.state_dict(),\n",
        "            'teacher_state_dict': self.teacher.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scaler_state_dict': self.scaler.state_dict() if self.use_amp else None,\n",
        "            'loss': loss,\n",
        "            'best_loss': self.best_loss,\n",
        "            'base_lr': self.base_lr,  # Save base learning rate\n",
        "        }\n",
        "\n",
        "        # Save regular checkpoint\n",
        "        checkpoint_path = self.out_dir / f\"checkpoint_epoch_{self.epoch}.pth\"\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "        # Save best checkpoint\n",
        "        if is_best:\n",
        "            best_path = self.out_dir / \"best_checkpoint.pth\"\n",
        "            torch.save(checkpoint, best_path)\n",
        "            logger.info(f\"New best checkpoint saved with loss: {loss:.4f}\")\n",
        "\n",
        "    def load_checkpoint(self, checkpoint_path):\n",
        "        \"\"\"Load training checkpoint\"\"\"\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
        "\n",
        "        self.epoch = checkpoint['epoch']\n",
        "        self.global_step = checkpoint['global_step']\n",
        "        self.best_loss = checkpoint['best_loss']\n",
        "        self.base_lr = checkpoint.get('base_lr', 0.0005)  # Load base learning rate\n",
        "\n",
        "        self.student.load_state_dict(checkpoint['student_state_dict'])\n",
        "        self.teacher.load_state_dict(checkpoint['teacher_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        if self.use_amp and checkpoint['scaler_state_dict'] is not None:\n",
        "            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
        "\n",
        "        logger.info(f\"Loaded checkpoint from epoch {self.epoch} with loss {checkpoint['loss']:.4f}\")"
      ],
      "metadata": {
        "id": "NdZ9JhGRrp0I"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class DINOLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    DINO loss function implementation.\n",
        "\n",
        "    This loss function implements the self-distillation with no labels approach\n",
        "    used in the DINO paper. It consists of:\n",
        "    1. Cross-entropy loss between student and teacher outputs\n",
        "    2. Centering of teacher outputs to avoid collapse\n",
        "    3. Sharpening of teacher distributions with temperature\n",
        "\n",
        "    Args:\n",
        "        out_dim (int): Output dimension of the projection head\n",
        "        warmup_teacher_temp (float): Initial teacher temperature\n",
        "        teacher_temp (float): Final teacher temperature (after warmup)\n",
        "        warmup_teacher_temp_epochs (int): Number of warmup epochs for teacher temperature\n",
        "        nepochs (int): Total number of epochs\n",
        "        student_temp (float): Student temperature\n",
        "        center_momentum (float): Momentum for center update\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, out_dim, warmup_teacher_temp=0.04, teacher_temp=0.07,\n",
        "                 warmup_teacher_temp_epochs=30, nepochs=100, student_temp=0.1,\n",
        "                 center_momentum=0.9):\n",
        "        super().__init__()\n",
        "        self.student_temp = student_temp\n",
        "        self.center_momentum = center_momentum\n",
        "        self.nepochs = nepochs\n",
        "        self.warmup_teacher_temp_epochs = warmup_teacher_temp_epochs\n",
        "\n",
        "        # Register buffer for center\n",
        "        self.register_buffer(\"center\", torch.zeros(1, out_dim))\n",
        "\n",
        "        # Temperature scheduling\n",
        "        self.warmup_teacher_temp = warmup_teacher_temp\n",
        "        self.teacher_temp = teacher_temp\n",
        "\n",
        "    def forward(self, student_output, teacher_output, epoch):\n",
        "        \"\"\"\n",
        "        Forward pass of the DINO loss.\n",
        "\n",
        "        Args:\n",
        "            student_output: List of student outputs for different crops\n",
        "            teacher_output: List of teacher outputs for different crops\n",
        "            epoch: Current epoch number (for temperature scheduling)\n",
        "\n",
        "        Returns:\n",
        "            Loss value\n",
        "        \"\"\"\n",
        "        # Get current teacher temperature (with warmup)\n",
        "        teacher_temp = self.get_teacher_temp(epoch)\n",
        "\n",
        "        # Gather all outputs\n",
        "        student_out = self.gather_outputs(student_output)\n",
        "        teacher_out = self.gather_outputs(teacher_output).detach()  # Detach early\n",
        "\n",
        "        # Apply temperature to student outputs\n",
        "        if isinstance(student_output, list):\n",
        "            student_output = torch.cat(student_output, dim=0)\n",
        "            teacher_output = torch.cat(teacher_output, dim=0)\n",
        "        student_out = student_out / self.student_temp\n",
        "\n",
        "        # Apply temperature and center to teacher outputs\n",
        "        teacher_out = (teacher_out - self.center) / teacher_temp\n",
        "        teacher_out = F.softmax(teacher_out, dim=-1)\n",
        "\n",
        "        total_loss = 0\n",
        "        n_loss_terms = 0\n",
        "        for iq, q in enumerate(teacher_out):\n",
        "            for v in range(len(student_out)):\n",
        "                if v == iq:\n",
        "                    # we skip cases where student and teacher operate on the same view\n",
        "                    continue\n",
        "                loss = torch.sum(-q * F.log_softmax(student_out[v], dim=-1), dim=-1)\n",
        "                total_loss += loss.mean()\n",
        "                n_loss_terms += 1\n",
        "        total_loss /= n_loss_terms\n",
        "        self.update_center(teacher_output)\n",
        "        return total_loss\n",
        "\n",
        "    def get_teacher_temp(self, epoch):\n",
        "        \"\"\"Get teacher temperature with warmup schedule\"\"\"\n",
        "        if epoch < self.warmup_teacher_temp_epochs:\n",
        "            # Linear warmup\n",
        "            return self.warmup_teacher_temp + (self.teacher_temp - self.warmup_teacher_temp) * \\\n",
        "                   epoch / self.warmup_teacher_temp_epochs\n",
        "        else:\n",
        "            return self.teacher_temp\n",
        "\n",
        "    def gather_outputs(self, outputs):\n",
        "        \"\"\"\n",
        "        Gather outputs from all crops and concatenate them.\n",
        "        \"\"\"\n",
        "        return torch.cat([output for output in outputs], dim=0)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_center(self, teacher_output):\n",
        "        \"\"\"\n",
        "        Update center used for teacher output centering.\n",
        "        \"\"\"\n",
        "        # Gather all teacher outputs (raw, before temperature/softmax)\n",
        "        teacher_out = self.gather_outputs(teacher_output)\n",
        "\n",
        "        # Calculate batch mean\n",
        "        batch_center = torch.mean(teacher_out, dim=0, keepdim=True)\n",
        "\n",
        "        # Update center with momentum\n",
        "        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize models (using your VisionTransformerWrapper)\n",
        "    model_name = \"vit_small_patch16_224\"\n",
        "    img_size = 224\n",
        "    std_img_size = 224\n",
        "    out_dim = 1024\n",
        "\n",
        "    # Student model (trainable)\n",
        "    student_model = VisionTransformerWrapper(\n",
        "        model_name=model_name,\n",
        "        img_size=std_img_size,\n",
        "        pretrained=True,\n",
        "        is_teacher=False\n",
        "    )\n",
        "\n",
        "    # Teacher model (frozen, updated via EMA)\n",
        "    teacher_model = VisionTransformerWrapper(\n",
        "        model_name=model_name,\n",
        "        img_size=img_size,\n",
        "        pretrained=True,\n",
        "        is_teacher=True\n",
        "    )\n",
        "    # Example configuration (adjust values based on your setup)\n",
        "    ncrops = 6  # 2 global crops + 4 local crops\n",
        "    warmup_teacher_temp = 0.04\n",
        "    teacher_temp = 0.07\n",
        "    warmup_teacher_temp_epochs = 30\n",
        "    nepochs = 100\n",
        "\n",
        "    # Initialize DINOLoss with the required arguments\n",
        "    loss_fn = DINOLoss(\n",
        "        #ncrops=ncrops,\n",
        "        warmup_teacher_temp=warmup_teacher_temp,\n",
        "        teacher_temp=teacher_temp,\n",
        "        warmup_teacher_temp_epochs=warmup_teacher_temp_epochs,\n",
        "        nepochs=nepochs,\n",
        "        out_dim=1024\n",
        "    )\n",
        "\n",
        "    # Create COCO data loader\n",
        "    dataDir = \"/content/object-detection-BBD/data/100k/test\"\n",
        "    annFile = f\"{dataDir}/_annotations.coco.json\"\n",
        "    dataloader = create_coco_dataloader(\n",
        "        annFile=annFile,\n",
        "        dataDir=dataDir,\n",
        "        batch_size=4,\n",
        "        num_workers=4,\n",
        "        global_crop_size=224,\n",
        "        local_crop_size=96,\n",
        "    )\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = optim.AdamW(\n",
        "        student_model.parameters(),\n",
        "        lr=0.0005,\n",
        "        weight_decay=0.04\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = DINOTrainer(\n",
        "        student_model=student_model,\n",
        "        teacher_model=teacher_model,\n",
        "        dataloader=dataloader,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        out_dir=\"./dino_coco_checkpoints\",\n",
        "        warmup_epochs=1,\n",
        "        total_epochs=10,\n",
        "        save_freq=10,\n",
        "        use_amp=True,\n",
        "        # use_ddp=False  # Set to True for multi-GPU training\n",
        "    )\n",
        "\n",
        "    # Start training\n",
        "    trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "0beV7UUzuJsp",
        "outputId": "8334aff7-afe7-40b8-a3d6-c34a0f247b38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4019201934.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDINOLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mDINO\u001b[0m \u001b[0mloss\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mimplementation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mloss\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mimplements\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdistillation\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mno\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0mapproach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1M8KBETzuKrO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}